---
title: "The Dominance of Ones: A Handy Quirk of Numbers"
slug: "the-dominance-of-ones"
date: "2023-11-26"
authors:
  - author: <a href="/ryan">Ryan Alyn Porter</a>
  - author: and the <a href="https://chat.openai.com/g/g-MA4ZbLQBi-assistant-alpha"><i>Anthus</i></a> GPT
tags: ["mathematics", "statistical phenomena", "simulation theory"]
excerpt: From money launderers to breeding rabbits, this facsinating quirk of numbers is lurking on the sidelines, ready to surprise us.
preview_image: "./images/benfords-law/border-crossing-counts.png"
images:
  - ./images/benfords-law/border-crossing-counts.png
  - ./images/benfords-law/connecticut-real-estate-sales.png
  - ./images/benfords-law/new-york-baby-names.png
  - ./images/benfords-law/nyc_covid19_cases.png
  - ./images/benfords-law/connecticut-school-attendance.png
  - ./images/benfords-law/linear-and-compound-interest-growth-curves.png
  - ./images/benfords-law/compound_lead_digits.png
  - ./images/benfords-law/linear_lead_digits.png
  - ./images/benfords-law/simulated-bank-account-deposits---individual-transactions-.png
  - ./images/benfords-law/simulated-bank-account-deposits---daily-sums.png
  - ./images/benfords-law/simulated-1%-structured-deposits---individual-transactions-.png
  - ./images/benfords-law/simulated-1%-structured-deposits---daily-sums.png
  - ./images/benfords-law/chi-square-statistic-comparison.png
  - ./images/benfords-law/ks-statistic-for-benford's-law.png
  - ./images/benfords-law/kl-divergence-comparison.png
  - ./images/benfords-law/linear_increase_plot.png
  - ./images/benfords-law/logarithmic_increase_plot.png
  - ./images/benfords-law/log-tables.jpg
---

import BlogImage from '../components/blog-image';

Want to see something weird about numbers?  There's a useful quirk about how numbers from natural datasets look that's easy to see.  Hold onto your calculators!  This is going to get a little strange.

## Lead digit analysis

There are a lot of open datasets out there.  I went surfing around on [Data.gov](https://data.gov) and picked one mostly at random.  [This dataset](https://catalog.data.gov/dataset/border-crossing-entry-data-683ae) is from the federal government, and it's a count of the number of border crossings there were for different time periods, broken down by port of entry and by category, like "trains", "trucks", "bus passengers", "pedestrians."  Here's what the raw data looks like:

<table className='data'>
  <thead>
    <tr>
      <th>Port</th>
      <th>Date</th>
      <th>Measure</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Roma, Texas</td>
      <td>May 2017</td>
      <td>Trucks</td>
      <td className='number'>760</td>
    </tr>
    <tr>
      <td>Roma, Texas</td>
      <td>May 2012</td>
      <td>Trains</td>
      <td className='number'>0</td>
    </tr>
    <tr>
      <td>Piegan, Montana</td>
      <td>Mar 2020</td>
      <td>Trucks</td>
      <td className='number'>168</td>
    </tr>
    <tr>
      <td>Roma, Texas</td>
      <td>Dec 2021</td>
      <td>Bus Passengers</td>
      <td className='number'>128</td>
    </tr>
    <tr>
      <td>Roma, Texas</td>
      <td>Mar 2021</td>
      <td>Trucks</td>
      <td className='number'>2661</td>
    </tr>
    <tr>
      <td>Roma, Texas</td>
      <td>Aug 2021</td>
      <td>Buses</td>
      <td className='number'>14</td>
    </tr>
    <tr>
      <td>Westhope, North Dakota</td>
      <td>Jul 2023</td>
      <td>Trucks</td>
      <td className='number'>105</td>
    </tr>
    <tr>
      <td>Warroad, Minnesota</td>
      <td>Jun 2022</td>
      <td>Trucks</td>
      <td className='number'>401</td>
    </tr>
    <tr>
      <td>Richford, Vermont</td>
      <td>May 2021</td>
      <td>Train Passengers</td>
      <td className='number'>24</td>
    </tr>
    <tr>
      <td>Sasabe, Arizona</td>
      <td>Mar 2023</td>
      <td>Pedestrians</td>
      <td className='number'>45</td>
    </tr>
    <tr className='etc'>
      <td>...</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>

Let's look at <mark>the first digit of each of those counts.</mark>  See that "760" for the first count?  The lead digit is "7".  Here's the lead digit for each of the above counts:

<table className='data'>
  <thead>
    <tr>
      <th>Border Crossings Count</th>
      <th>Lead Digit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td className='number'>760</td>
      <td className='number'>7</td>
    </tr>
    <tr>
      <td className='number'>0</td>
      <td className='number'>0</td>
    </tr>
    <tr>
      <td className='number'>168</td>
      <td className='number'>1</td>
    </tr>
    <tr>
      <td className='number'>128</td>
      <td className='number'>1</td>
    </tr>
    <tr>
      <td className='number'>2661</td>
      <td className='number'>2</td>
    </tr>
    <tr>
      <td className='number'>14</td>
      <td className='number'>1</td>
    </tr>
    <tr>
      <td className='number'>105</td>
      <td className='number'>1</td>
    </tr>
    <tr>
      <td className='number'>401</td>
      <td className='number'>4</td>
    </tr>
    <tr>
      <td className='number'>24</td>
      <td className='number'>2</td>
    </tr>
    <tr>
      <td className='number'>45</td>
      <td className='number'>4</td>
    </tr>
    <tr className='etc'>
      <td className='number'>...</td>
      <td></td>
    </tr>
  </tbody>
</table>

Now, think about the whole dataset.  This one has 38,549 entries.  Each of those entries has a lead digit.  Let's filter out the "0" counts and ignore those.  What percentage of those entries would you expect have "1" for a lead digit?

There are nine possible digits if we exclude "0" as an option.  So, would you think that "1" would occur as the lead digit about one ninth of the time?

<mark>I would imagine</mark> that if we were to count all the measurements where the first digit was "1", and then count all the measurements where the first digit is "2", and so on, that we would find about the same number of "1"s and "5"s.  After all, this data is as random as it gets.  So, the lead digit should be essentially random, right?

Let's confirm that assumption...

I made a Python notebook that you can run yourself that counts the number of measurements in the border-crossing dataset that start with "1", and with "2", and so on.  Here's the distribution:

<BlogImage images={props.pageContext.frontmatter.images} index={0} className="" alt="The distribution of lead digits from federal border crossing counts." />

Well...  <mark>Okay, that's weird.</mark>

There are a lot more 1s.  Shouldn't they all be even?  Shouldn't there be as many counts starting with a 5 as with a 1?  What's going on?

## Benford's Law: Expect a lot of ones as leading digits in some data

This phenomenon is known as Benford's Law, and it's more common than you might think. Benford's Law states that <mark>in many naturally occurring datasets, the first digit is more likely to be lower than higher.</mark> Specifically, '1' appears as the first digit about 30% of the time, while '9' appears as the first digit only about 5% of the time.

It's a pattern that emerges naturally in a <mark>wide range of data</mark>. From financial records to geographical data, Benford's Law often holds true.  Here are some other datasets that I found on Data.gov where you can easily see it:

Here's the lead-digit breakdown from about a million real estate [sale prices](https://catalog.data.gov/dataset/?q=&sort=views_recent+desc):

<BlogImage images={props.pageContext.frontmatter.images} index={1} className="" alt="The distribution of lead digits from real estate sale prices." />

The dots superimposed over the bars show the proportions predicted by Benford's Law.  They don't match exactly, but <mark>they do match the overall pattern</mark>.

Here's a list of [baby names](https://catalog.data.gov/dataset/popular-baby-names), ranked by popularity, based on civil birth registrations.  Each name has a count: How many registrations there were for each name.

<BlogImage images={props.pageContext.frontmatter.images} index={2} className="" alt="The distribution of lead digits from real estate sale prices." />

We all remember the daily counts of COVID-19 cases.  Here's the lead-digit breakdown on the daily [New York City case counts]():

<BlogImage images={props.pageContext.frontmatter.images} index={3} className="" alt="The distribution of lead digits from NYC daily COVID-19 case counts." />

And here's the lead-digit breakdown from a bunch of different [school attendance counts](https://catalog.data.gov/dataset/school-attendance-by-student-group-and-district-2021-2022):

<BlogImage images={props.pageContext.frontmatter.images} index={4} className="" alt="The distribution of lead digits from school attendance counts." />

## An astronomer makes a discovery.  About numbers.

Back in 1881, when the closest thing to a pocket calculator was a quick-witted accountant with a pencil, Simon Newcomb, an astronomer by trade, made a discovery that had nothing to do with stars but everything to do with digits. Newcomb's lightbulb moment didn't strike while stargazing or deep in complex calculations. Instead, it happened as he leafed through a well-thumbed logarithmic table book. In the days before digital convenience, these tables were the bread and butter of calculation, sort of like an ancient Excel spreadsheet but without the fun of accidental cell misformatting.

<BlogImage images={props.pageContext.frontmatter.images} index={17} className="" alt="Worn pages in a logarith table book." />

What caught Newcomb's eye was the curious case of the weary pages. The early sections of the logarithm book, <mark>the pages with numbers starting with '1', showed signs of significant wear and tear</mark>. It was as if these pages had run a marathon, while the latter ones looked barely out of the starting blocks. This wasn't just a case of page discrimination; it hinted at a numerical pattern. Newcomb theorized that numbers starting with '1' were not just the wallflowers of the numerical dance but were, in fact, the belle of the ball, getting picked far more often than their higher-digit counterparts.

Forming his hypothesis from this observation, Newcomb proposed that the first significant digit in many datasets is more likely to be '1' than any other number, with the chances diminishing as you count up to '9'.

It wasn't until 1938 that the idea gained widespread recognition, thanks to physicist Frank Benford. Benford expanded on Newcomb's observation, conducting a more thorough and extensive study that covered a diverse range of datasets. He demonstrated that the first-digit distribution he observed closely matched Newcomb's earlier findings. Despite Newcomb's precedence, the law came to be known as Benford's Law due to his role in popularizing and formally documenting the phenomenon. 

## Okay, "some data"?

Let's look at a dataset where it doesn't happen.  Here are two charts of two different bank account balances.  Both grew over ten years from a principal of USD $10,000 to a final amount of USD $1,000,000.  One of them grew as a result of compounded interest on the principal.  The other grew in simple linear steps each month so that it ended up at the same point.

<BlogImage images={props.pageContext.frontmatter.images} index={5} className="" alt="A linear growth rate and a compound-interest growth rate." />

If we do the same kind of analysis where we look at the breakdown of the first digits of the monthly bank balances in both bank accounts, we see Benford's Law reflected in the distribution for the compound interest, like in the natural datasets that we saw:

<BlogImage images={props.pageContext.frontmatter.images} index={6} className="" alt="The lead-digit breakdown for a simulated compound interest growth rate." />

But, look at the lead-digit distribution for the bank account balances in the account that grew linearly over time:

<BlogImage images={props.pageContext.frontmatter.images} index={7} className="" alt="The lead-digit breakdown for a simulated linear growth rate." />

One kind of growth rate shows the kind of distribution described by Benford's Law, and the other doesn't.  <mark>Something that grows like an investment</mark>, exponentially, will show that kind of distribution.  Someone regularly transferring money looks different.

Well gosh, <mark>that sounds useful.</mark>

## Applications

Benford's Law's predictions have powerful applications in financial forensics.  It predicts what financial numbers should 'naturally' look like, and that gives us a way to spot numbers that someone manipulated.

For example, money launderers trying to move money from one point to another without being detected will send money below certain amounts that trigger extra scrutiny.  A common scenario might be a money launderer with $35,000 to transfer who wants to avoid a $10,000 limit that will trigger additional reporting.  They might break up their transactions into smaller chunks, like $9,500, $9,000, $8,500, and $8,000.  Benford's Law can spot that in the deposit records if you have enough data and it's happening often enough.

In other words, money laundering looks different than legitimate transactions.  It's like the numbers come with a <mark>built-in lie detector</mark>.  Handy!

## Let's simulate money laundering!

Imagine that you're a financial regulator and you're monitoring two banks.  You only have access to the total amount of deposits for each day from each bank.  There are a thousand deposits each day at each bank that are below the $10,000 reporting threshold, but someone is slipping structured transactions through one of the banks.  Your job is to figure out which bank.

I made a Python notebook that simulates that.  It randomly generates a thousand transactions for each of 365 days, and then it adds up the transactions for each day.  We get a list that contains the total sums of deposits for each of the 365 days.  Here's the lead digit breakdown for that list of simulated daily desposit totals:

<BlogImage images={props.pageContext.frontmatter.images} index={9} className="" alt="The lead-digit breakdown for the natural transactions scenario." />

Now, we simulate a second scenario: One where someone is making structured deposits.  In the second scenario, we replace 1% of the natural transaction amounts with simulated structured transaction amounts.  Our simulation is simple, they all randomly range from $9,500 to $9,999.  We get another 365 daily sums, and here's the lead digit breakdown for that scenario:

<BlogImage images={props.pageContext.frontmatter.images} index={11} className="" alt="The lead-digit breakdown for the structured transactions scenario." />

Does one of those distributions look "closer" to the distribution predicted by Benford's Law?  Well, I don't know, they look abou the same to me.  Is there some way to measure it?

Yes!  There are a few common ways to measure the difference that are used in things like financial forensics.  There's also one used a lot in AI.

### Chi-Square Statistic

The [Chi-Square](https://en.wikipedia.org/wiki/Chi-squared_test) statistic is a measure used to determine how well an observed distribution of data fits an expected distribution. In our case, we use it to compare the distribution of the first digits of our transaction data against the expected Benford's Law distribution.  We expect this number to be higher if the distribution varies more from the distribution predicted by Benford's Law.  Let's look at this statistic for both the simulated 'natural' transactions and the scenario with structured transactions buried inside:

<BlogImage images={props.pageContext.frontmatter.images} index={12} className="" alt="The Chi-Square Statistic for both scenarios." />

Wow, neat,  It is higher for the structured transactions.

We can see which bank has money launderers for customers!  They might not even know, but we can see it <mark>just from looking at the daily sums of deposits</mark>, without being able to see the individual transactions.  How cool!

### Kolmogorov-Smirnov (KS) Statistic

The [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test) (KS) statistic is another metric that measures the difference between two distributions.  Here's how the two scenarios compare:

<BlogImage images={props.pageContext.frontmatter.images} index={13} className="" alt="Kolmogorov-Smirnov (KS) Statistic for both scenarios." />

That one <mark>also shows</mark> the money launderers varying significantly from the normal transactions!

### Kullback-Leibler (KL) Divergence

[Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) is a loss measure is used a lot in AI for things like RLHF evaluation, so why not...

<BlogImage images={props.pageContext.frontmatter.images} index={14} className="" alt="Kullback-Leibler (KL) Divergence for both scenarios." />

Cool, right?  For all three of those measures, the scenario where <mark>just 1% of the transactions buried within an aggregate sum</mark> scored higher than the simulated natural number dataset.  We can see that there are structured transactions buried in that data without being able to see the indivdual transactions, only the daily summaries.

## But why does Benford's Law work?

Let's see if we can find the answer in Rabbit Utopia, a safe and cozy place where the furry inhabitants never have to worry about lettuce rations.  Let's think about the population of Rabbit Utopia.  And specifically about the <mark>number of digits</mark> of the population count.

We start with six rabbits in Generation 1, a simple one-digit figure. As we observe their growth, doubling each generation, the change in the number of digits tells a story.

By Generation 2, we reach 12 rabbits, now a two-digit number.  But it's on the low side of the two-digit numbers.  Why?  It's because as we cross into the next order of magnitude, the bucket of numbers that start with "1" is as big as the bucket of all the numbers that came before it.  If you were jogging along at a steady pace then you would be cranking up in digits as you went from 70 to 80 to 90 but then you would be stuck in the 100s for as long as it took you to get here.  But fortunately, rabbits don't breed at a steady pace, they breed at an exponential rate.

By Generation 6, the population hits 192, marking our first transition to a three-digit number.  We skipped a lot of lead digits in the two-digit numbers, but yet again we're stuck in the low-lead-digit numbers, since there are as many three-digit numbers starting with 1 as there were numbers before us.  When we cross to this next order of magnitude, we're still at the lower end of it.  Since, the bucket of three-digit numbers that starts with "1" is as big as the bucket of all the numbers that came before it.

Let's take a look at the relationship between the population value and the <mark>number of digits</mark> in the population count, all the way up to 2 billion:

<BlogImage images={props.pageContext.frontmatter.images} index={15} className="" alt="Linear growth to two billion." />
<BlogImage images={props.pageContext.frontmatter.images} index={16} className="" alt="Logarithmic growth in number of digits to two billion." />

The first chart illustrates the linear growth in population value, while the second chart illustrates the stepwise logarithmic growth pattern in the number of digits, contrasting the linear population increase against the more gradual digit count rise.

These visualizations demonstrate the essence of Benford's Law: in a logarithmic scale, lower digits dominate because <mark>each digit covers a wider range before the next takes over.</mark>

Another interesting thing about Benford's Law: You <mark>don't really have to understand why it happens</mark> to understand how to use it.

## Conclusion

Benford's Law, with its <mark>unique insight into aggregate data</mark>, already plays a role in various applications, from fraud detection to financial analysis. However, its potential extends further, particularly in shaping the future of AI.

One intriguing application lies in crafting datasets for training AI models. By ensuring these datasets adhere to Benford's Law, we could enhance the realism and reliability of the training process, leading to more robust and accurate AI models. Benford's Law could be used in areas like evaluation metrics in AI, offering a novel lens to assess the authenticity and accuracy of AI-generated data.

While its current applications are well-established, the future might see Benford's Law becoming a <mark>part of the AI toolkit</mark>, guiding both the development and evaluation of AI models in a variety of fields. This exploration could pave the way for AI systems that resonate more deeply with the patterns found in natural data.

## Lab

Don't take my word for it.  <mark>You can reproduce all of my findings and visuals</mark> from this article with Python notebooks on Google Collab.

### Data.gov datasets

You can download datasets from [Data.gov](https://data.gov) to Collab and do your own lead-digit analysis with this notebook: [Benford's Law Demo](https://colab.research.google.com/drive/1NnENSOTvp-7ts_f2Oj9AwUEYummD--YD?usp=sharing)

<mark>Play with it</mark>: Find your own datasets and try it.  Find any dataset that you think might work and copy the CSV download link to the notebook.  Download the file yourself ahd look at it to find the column name of the column that you think might demonstrate a distribution that follows the prediction from Benford's Law.

### Money laundering simulation

You can use the notebook for the [structured-transaction simulation](https://colab.research.google.com/drive/1MV45hNw86qe6GUnTtCsnKxcXMwocsTfg?usp=sharing) to reproduce my findings.  Run the simulation a few times and look at the different values that you get for the three different measures of how the actual distribution varies from the Benford's Law predicted distribution for the two different scenarios.  <mark>Random numbers are random</mark>: The statistics won't always magically detect every money launderer.  Sometimes they'll even go backwards.  Random!

Then, <mark>change the parameters</mark>.  Instead of a thousand transactions per day, lower it to 100.  Lower the percentage of structured transactions.  How low can you go before the differing lead-digit distributions are imperceptible in terms of how they correspond with the Benford's Law predictions?  If you increase the number of structured transactions then does it enhance the distinction?